{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ccd8e7a",
   "metadata": {},
   "source": [
    "# NEWS SUMMARIZATION USING SEQ2SEQ\n",
    "THE PURPOSE OF THIS PROJECT IS TO BUILD A MODEL THAT SUMMARIZES NEWS ARTICLES USING DEEP LEARNING .THE SEQ 2 SEQ MODEL \n",
    "INCUDES AN ENCODER AND DECODER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from torchtext.data import Field,Iterator,BucketIterator,Example,Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.snowball import SnowBallStemmer\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04092a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we do some text data processing\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self,path,fields,**kwargs):\n",
    "        # we initialize the fields\n",
    "        if not isinstance(fields[0](tuple,list)):\n",
    "            fields = [('src'fields[0]),('trg',fields[1])]\n",
    "            self.new_list = self.read_data()\n",
    "            # To read news articles and summarize them in a pandas dataframe\n",
    "            examples = [Example.fromlist(list(item),fields)for item in self_new_list]\n",
    "            # the above code is so that our model reads news items as torch text examples\n",
    "        super().__init__(examples,fields,**kwargs)\n",
    "    def __len__(self):\n",
    "        try :\n",
    "            return len(self.examples)\n",
    "        except TypeError:\n",
    "            return 2**32\n",
    "    def __get__item(self,index):\n",
    "        return self.examples[index]\n",
    "    def __iter__(self):\n",
    "        for x in self.examples:\n",
    "            yield x\n",
    "    def __getattr__(self,atrr):\n",
    "        if attr in self.fields:\n",
    "            for x in self.examples:\n",
    "                yield getattr(x,atrr)\n",
    "    def read_data(self):\n",
    "        Articles = []\n",
    "        Summaries = []\n",
    "        for d,path,filenames in tqdm(os.walk(self.path)):\n",
    "            for file in filenames:\n",
    "                if os.path.isfile(d+'/'+file):\n",
    "                    if ('Summaries') in (d+'/'+file):\n",
    "                        with open(d+'/'+file,errors ='ignore') as f:\n",
    "                            summary = \n",
    "                            \n",
    "                            ''.join([i.rstrip() for i in f.readlines[i]])\n",
    "                            Summaries.append(summary)\n",
    "                 else :\n",
    "                    with open(d+'/'+file,'r',errors='ignore') as f:\n",
    "                        articles = ''.join([i.rstrip() for i in f.readlines(i)])\n",
    "                        Articles.append(articles)\n",
    "            return zip(Articles,Summaries)\n",
    "    #  A functions that cleans text data \n",
    "    def clean_data(self.text):\n",
    "        text = self._remove_links(text)\n",
    "        text = self._remove_numbers(text)\n",
    "        text = self._remove_punct(text)\n",
    "        return text.lower\n",
    "    def _remove_punct(self,text):\n",
    "        no_punct = ''\n",
    "        for c in text:\n",
    "            if c not in string.punctuation:\n",
    "                no_punct+=c\n",
    "        return no_punct\n",
    "    def _remove_numbers(self,text) :\n",
    "        return re.sub(r'[0-9]','',text)\n",
    "    def _remove_links(self,text):\n",
    "        return re.sub(r'http\\s+','',text)\n",
    "    def _get_root(self,word_list):\n",
    "        ps= PorterStemmer()\n",
    "        return [ps.stem for word in word_list]\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aa46e",
   "metadata": {},
   "outputs": [],
   "source": [
    " def tokenize_en(self,text):\n",
    "        # Using spacy tokenizer\n",
    "        #first instantiating the tokenizer\n",
    "        spacy_eng = spacy.load('en')\n",
    "        return [tok.text for tok in spacy_en.tokenize(text)]\n",
    "SRC = Field(tokenizer = tokenizer_eng,\n",
    "           init_token ='<sos>',\n",
    "           eos_token = '<eos>',\n",
    "           fix_length = 500\n",
    "           lower =True)\n",
    "TRG = Field(tokenizer = tokenize_eng,\n",
    "           init_token ='<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            fix_lenght = 200\n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44763bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = NewsDataset(path='/kaggle/input',fields = [SRC,TRG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into train,valid and test sets\n",
    "train_data,valid_data,test_data = news_data.split(split_ratio=[0.8,0.1,0.1]random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building our vocabularies\n",
    "SRC.build_vocab(train_data,min_freq=2)\n",
    "TRG.build_vocab(train_data,min_freq=2)\n",
    "# I am going to create two variables ,one for src len and another for trg len\n",
    "# Using the variables i will create a function that returns the lenght of the tokens\n",
    "\n",
    "def get_item_lenght(data):\n",
    "    src_len = []\n",
    "    trg_len = []\n",
    "    for item in data.examples:\n",
    "        src_len.append(len(vars(item))['src']\n",
    "        trg_len.append(len(vars(item))['trg']\n",
    "    return src_len,trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a89d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an iterator since we are goin to be using batch sizes\n",
    "train_iterator,valid_iterator,text_iterator = Iterator.splits((train_data,valid_data,test_data),batch_size=64,sort key = lambda x:len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e203ae7",
   "metadata": {},
   "source": [
    "# BUILDING OF THE ENCODER,DECODER AND THE SEQ2SEQ MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b38f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,emd_dim,hid_dim,n_layer,dropout=0.2,bidirection=True):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_dim = nn.Embedding(input_dim,emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim,hid_dim,n_layers,bidirection=bidirection,dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,src):\n",
    "        embedded = self.dropout(self.embedded(src))\n",
    "        outputs,(hidden,cell) = self.lstm(embedded)\n",
    "        return outputs,hidden,cell\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,emb_dim,hid_dim,n_layers,bidirection=True,dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim,emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim,hid_dim,n_layers,dropout=dropout,bidirection=bidirection)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # A linear layer for our output will be a good fit since its about predicting words\n",
    "        self.fc = nn.Linear(hid_dim,output_dim)\n",
    "    def forward(self,trg):\n",
    "        trg = trg.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(trg))\n",
    "        output,(hidden,cell) = self.lstm(embedded)\n",
    "        prediction = self.fc(output).squeeze(0)\n",
    "        return prediction,hidden,cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87161f",
   "metadata": {},
   "source": [
    "# SEQ2SEQ MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2cd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    def forward(self,src,trg,teacher_forcing_ratio= 0.5):\n",
    "        # First we define the batch size \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len  = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # Creating a decoder to store our tensor units\n",
    "        outputs = torch.zeros(trg_len,batch_size,trg_vocab_size)\n",
    "        hidden,cell = self.encoder(src)\n",
    "        dec_input = trg[0,:] # to get the sos token only\n",
    "        for t in range(1,trg_len):\n",
    "            output,hidden,cell = self.decoder(dec_input,hidden,cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.randn() > teacher_forcing_ratio\n",
    "            # we use top 1 to get the top output\n",
    "            top1 = output.argmax(1)\n",
    "            dec_input =trg[t] if teacher_force else top1\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "enc_emb_dim = 128\n",
    "dec_emb_dim = 128\n",
    "hid_dim = 256\n",
    "n_layers = 2\n",
    "enc = Encoder(input_dim,emb_dim,hid_dim,n_layers)\n",
    "dec = Decoder(output_dim,emb_dim,hid_dim,n_layers)\n",
    "model = Seq2Seq(enc,dec,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242093b",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bff518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Seq_Trainer(object):\n",
    "    def __init__(self,model,train_iterator,valid_iterator,pad_idx,device,clip,learning_rate):\n",
    "        self.model = model\n",
    "        self.train_iterator = train_iterator\n",
    "        self.valid_iterator = valid_iterator\n",
    "        self.clip =clip\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_idx= pad_idx)\n",
    "        self.model.apply(self.init_weights)\n",
    "    def init_weights(self,m):\n",
    "        for name, param in m.named_parameters():\n",
    "            nn.init_uniform(param.data,-0.08,0.08)\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in model.parameters()if p.requires_grad)\n",
    "    def train(self):\n",
    "        self.model.train\n",
    "        epoch_loss = 0.0\n",
    "        for i , batch in enumerate(self.train_iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            self.optimizer.zero_grad()\n",
    "            output = model(src,trg)\n",
    "            output_dim = output[1:,].view(-1,output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = self.criterion(output,trg)\n",
    "            loss.backwards()\n",
    "            torch.nn.utils.clip_grad_norm(self.model.parameters(),self.clip)\n",
    "            self.optimizer.step()\n",
    "            epoch_loss+=loss\n",
    "        return epoch_loss/len(self.train_iterator)\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        epoch_loss=0.0\n",
    "        for i,batch in enumerate(valid_iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(src,trg,0)\n",
    "            output_dim = output[1:].view(-1,output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            val_loss = self.criterion(output,trg)\n",
    "            val_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(self.model.parameters(),self.clip)\n",
    "            self.optimizer.step()\n",
    "            epoch_loss +=val_loss.item()\n",
    "        return epoch_loss/len(valid_iterator)\n",
    "    def fit(self,nepochs):\n",
    "        best_valid_loss = float('inf')\n",
    "        for epoch in range(nepochs):\n",
    "            start_time = time.time()\n",
    "            train_loss = self.train()\n",
    "            valid_loss = self.evaluate()\n",
    "            epoch_min,epoch_secs = (start_time,end_time)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "            print(f\"Epoch {epoch+1:02}|Time {epoch_mins}m {epoch_secs}s\")\n",
    "            print(f'Train Loss{train_loss} |Valid Loss{valid_loss}')\n",
    "    \n",
    "    def  predict(self,iterator):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i ,batch in enumerate(tqdm(iterator)):\n",
    "                src = batch.src\n",
    "                trg = batch.trg\n",
    "                outputs = self.model(src,trg,0) # This is to turn off the teacher forcing ratio\n",
    "                if i == 0:\n",
    "                    outputs = torch.argmax(output,-1)\n",
    "                else :\n",
    "                    outputs = torch.cat(outputs,torch.argmax(output,-1),dim=-1)\n",
    "            return torch.transpose(outputs,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can config some variables\n",
    "pad_idx = TRG.vocab.stoi(TRG.pad_tokens)\n",
    "trainer = Seq_Trainer(model,train_iterator,valid_iterator,pad_idx=pad_idx,1,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50339197",
   "metadata": {},
   "source": [
    "THANKS TO :: \n",
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "https://github.com/Mjkim88/Pytorch-Torchtext-Seq2Seq\n",
    "https://torchtext.readthedocs.io/en/latest/\n",
    "https://www.kaggle.com/mallaavinash/text-summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f944a",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d22bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

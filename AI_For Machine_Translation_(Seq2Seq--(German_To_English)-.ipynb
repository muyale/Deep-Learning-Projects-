{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d32a08",
   "metadata": {},
   "source": [
    " # AI FOR MACHINE TRANSLATION\n",
    "The purpose of this project iis to create an AI that translates languages . The project will be built on pytorch : the idea of neural networks and embeddings and will also include Spacy package for language tokenization and building vocabulary .\n",
    "\n",
    "The project is a Seq2Seq model with an encoder and  decoder which I will bulid from scratch . It is my first machine translation \n",
    "project and I hope it gives other AI enthusiasts a guide or inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb00d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries \n",
    "import torch\n",
    "from torch import nn,Tensor\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import time \n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataloader\n",
    "import torchtext\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torchtext.data import Field,BucketIterator\n",
    "from torchtext.datasets import IWSLT2016\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b25520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the datasets for german and english ,using spacy\n",
    "!python-m spacy download en_core_web_md>logs.txt\n",
    "!python -m spacy download de_core_news_md>logs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our dowloaded data\n",
    "spacy_en = spacy.load('en_core_web_md')\n",
    "spacy_de = spacy.load('de_core_new_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing our  text data\n",
    "sos_tok = '<sos>'\n",
    "eos_tok ='<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0033fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing our text data\n",
    "def preprocessing(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(f'[{string.punctuation}\\n]','',text)\n",
    "    return text\n",
    "def tokenize_german(text):\n",
    "    text  = preprocessing(text)\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "def tokenize_eng(text):\n",
    "    text = preprocessing(text)\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training Data,Validation_Data and Testing Data ,using torch.DataSets\n",
    "# First we accesss the Multi30K dataset\n",
    "train_data,valid_data,test_data = torchtext.datasets.Multi30k(root='data',split =('train','valid','split')\n",
    "                                                             ,language_pair = ('de','eng'))\n",
    "\n",
    "# Next we convert it into a real datasets ,because at first it is just text data \n",
    "# Dataset Class\n",
    "class TextDataSet(torch.utils.Datasets):\n",
    "    \"\"\"\n",
    "    * Parameters:\n",
    "      raw_data - raw_text data from our multi30k datasets]\n",
    "      trg - English tokens\n",
    "      src - German tokens\n",
    "    \"\"\"\n",
    "    def __init__(self,raw_data):\n",
    "        # I will convert raw_data into a list as a container to store the data\n",
    "        self.datasets = list(raw_data)\n",
    "    def __len___(self):\n",
    "        return len(self.data)\n",
    "    def __get__item__(self,idx) :\n",
    "        # This method uses indexing to access data \n",
    "        src,trg = self.datasets[idx]\n",
    "        src = [sos_tok] + tokenize_german(src)+[eos_tok]\n",
    "        trg = [sos_tok] + tokenize_english(trg) +[eos_tok]\n",
    "        # src are german language tokens ,trg are english language tokens\n",
    "        return src,trg\n",
    "train_datasets = TextDataSet(train_data)\n",
    "test_datasets  = TextDatset(test_data)\n",
    "valid_datasets = TextDataset(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next I will use pytorch lightning as a DataLoader\n",
    "class TextDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,train_data,valid_data,test_data,batch_size=128):\n",
    "        self.train_data = train\n",
    "        self.valid_data = valid\n",
    "        self.test_data = test \n",
    "        self.batch_size = 128\n",
    "    def train_data_loader(self):\n",
    "        return DataLoader(self.train,self.batch_size)\n",
    "    def test_data_loader(self):\n",
    "        return DataLoader(self.test,self.batch_size)\n",
    "    def valid_data_loader(self) :\n",
    "        return DataLoader(self.valid,self.batch_size)\n",
    "    return DataLoader(train_datasets,valid_datasets,test_datasets)\n",
    "batch_size = 128\n",
    "dm = TextDataModule(train_dataset,valid_datasets,test_datasets,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990ea37",
   "metadata": {},
   "source": [
    "# CORPUS BUILDING AND VOCABULARY BUILDING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will create a corpus on trg and src i.e English language and German _Language\n",
    "src_corpus = []\n",
    "trg_corpus = []\n",
    "for src,trg in train_datasets:\n",
    "    src_corpus.append(src)\n",
    "    trg_corpus.append(trg)\n",
    "\n",
    "# Creating a Vocabulary for src and eng language and including unknown tokens\n",
    "src_vocab = torchtext.vocab_build_vocab_from_iterator(src_corpus)\n",
    "trg_vocab = torchtext.vacab_build_vocab_from_iterator(trg_corpus)\n",
    "# For vocabulary with a frequency less than 1 \n",
    "data_src.build_vocab(train_data,min_freq=2)\n",
    "data_trg.build_vocab(train_data,min_freq =2)\n",
    "itos_src = data_src.vocab.itos\n",
    "itos_trg = data.trg.vocab.itos\n",
    "\n",
    "\n",
    "# A function that gets the set vocabulary :::\n",
    "def get(itos,i):\n",
    "    try :\n",
    "        return itos[i]\n",
    "    except :\n",
    "        ...\n",
    "        return '<unk>'\n",
    "    \n",
    "def decode_src(x:Tensor):\n",
    "    return ''.join([get(itos_src,i) for i in x])\n",
    "def decode_trg(x:Tensor):\n",
    "    return ''.join([get(itos_trg,i)]for i in x)\n",
    "# I will now use the Bucket Split iterator to iterate through our vocabulary \n",
    "batch_size = 128\n",
    "train_data,valid_data,test_data = BucketIterator.splits((train_data,valid_data,test_data),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21468d",
   "metadata": {},
   "source": [
    "# ENCODER AND DECODER USING NN.MODULE : RNN + EMBEDDING + LINEAR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,emb_dim,hid_dim,n_layers,dropout=0.4):\n",
    "        \"\"\"Args :\n",
    "        Parameters\n",
    "        input_dim -vocabulary size\n",
    "        emb_dim  - word_embedding_size\n",
    "        n_layers - number of hidden layers for the RNN\n",
    "        hid_d - Hidden state sizes \n",
    "        dropout - This is the dropout ratio to address overfitting\n",
    "        Returns :\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim,hidden_dim.n_layers,batch_first =True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,src:Tensor):\n",
    "        embedded = sself.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        out,(h,c) = self.rnn(embedded)\n",
    "        return h,c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,emb_dim,hidden_dim,n_layers,dropout = 0.4):\n",
    "        super().__init__()\n",
    "        self.embedding  = nn.Embedding(output_dim,emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim,hidden_dim,n_layers,batch_first =True)\n",
    "        self.fc_out = nn.Linear(hidden_dim,output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,trg:Tensor):\n",
    "        trg =trg.unsqueeze()\n",
    "        embedded = self.embedded(trg)\n",
    "        embedded = self.dropout(trg)\n",
    "        out,(h,c) = self.rnn(embedded,(encoder_h,encoder_c))\n",
    "        out = self.fc_out(out.squeeze())\n",
    "        return out,h,c\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04798cd",
   "metadata": {},
   "source": [
    "# Seq 2  Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f1603d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9972\\3540047860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "  \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    def forward(self, src, trg, teacher_force_ratio = 0.5):\n",
    "        \n",
    "        h, c = self.encoder(src)\n",
    "        \n",
    "        decoder_input = trg[:, 0] # just get the <sos> token\n",
    "        \n",
    "        # [src len, batch size, vocab size]\n",
    "        outputs = torch.zeros(trg.size(1), trg.size(0), self.decoder.fc_out.out_features)\n",
    "        \n",
    "        for t in range(1, trg.size(1)):\n",
    "            out, h, c = self.decoder(decoder_input, h, c)\n",
    "            \n",
    "            outputs[t] = out\n",
    "            \n",
    "            top1 = out.argmax(1)\n",
    "            \n",
    "            use_teacher_force = torch.rand() < teacher_force_ratio\n",
    "            \n",
    "            decoder_input = trg[:, t] if use_teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def prediction(self, src, max_len=50):\n",
    "        h, c = self.encoder(src)\n",
    "        sos_id = data_trg.vocab.stoi[sos_tok]\n",
    "        decoder_input = torch.tensor([[sos_id]])\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        outputs = self.forward(src, trg)\n",
    "        \n",
    "        output = output[1:].view(-1, output.size(-1))\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        outputs = self.forward(src, trg)\n",
    "        \n",
    "        output = output[1:].view(-1, output.size(-1))\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        outputs = self.forward(src, trg)\n",
    "    \n",
    "        output = output[1:].view(-1, output.size(-1))\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ff3bc",
   "metadata": {},
   "source": [
    "# MODEL TRAINING :::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df813bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input dim in our model is the number of vocabulary of thr german language ,the output dim is the number for English language\n",
    "input_dim = len(data_src.vocab)\n",
    "output_dim = len(data_trg.vocab)\n",
    "enc_emb_dim = 500\n",
    "dec_emb_dim = 500\n",
    "hid_dim = 512\n",
    "n_layers = 3\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "encoder = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout)\n",
    "decoder = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers, dec_dropout)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trg_pad_idx = data_trg.vocab.stoi[data_trg.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698b17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8769fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ef006",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "list_train_loss = []\n",
    "list_valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss = eval_epoch(model, valid_iterator, criterion)\n",
    "    \n",
    "    list_train_loss.append(train_loss)\n",
    "    list_valid_loss.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'nmt-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6035d",
   "metadata": {},
   "source": [
    "# MACHINE TRANSLATOR ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "\n",
    "    # Load german tokenizer\n",
    "    spacy_ger = spacy.load(\"de\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    # I will add the sos token at the start and eos token  in the  end respectively\n",
    "    tokens.insert(0, german.sos_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [english.vocab.stoi[sos_token]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == english.vocab.stoi[eos_token]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

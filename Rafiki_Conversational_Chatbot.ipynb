{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25c13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time  \n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f69fbda",
   "metadata": {},
   "source": [
    "checkpoint = 'microsoft/DialoGPT-medium'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM(checkpoint)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "849321c9",
   "metadata": {},
   "source": [
    "# ABOUT : RAFIKI\n",
    "The Idea behind this ChatBot is to create a virtual assistant or chatbot that a user can talk with.\n",
    "As we venture more into Artificial Intelligence there is going to be less and lesser interactions between humans.\n",
    "Virtual Assistants have already been deployed and are of importance .Examples include Siri,Alexa \n",
    "For my first ChatBot project I am going to build a general conversation type of AI using a pretrained Model.\n",
    "The Model(DialoGPT ) has been trained on over 147 millions conversations on Reddit and it has what many may term as \"cognitive\"\n",
    "Abilities to hold conversations.\n",
    "\n",
    "Lets Build Together ::\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d16dd",
   "metadata": {},
   "source": [
    "# CREATING THE CHATBOT CLASS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3596c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rafiki():\n",
    "    def __init__(self):\n",
    "        self.chat_history_ids = None # At the beginning of every conversation\n",
    "        self.bot_input_ids = None # I have made this a global variable\n",
    "        self.end_chat = False # A flag to check on the progress of the chat+ user\n",
    "        self.welcome()\n",
    "    def welcome(self):\n",
    "        print(\"Rafiki is Starting\")\n",
    "        time.sleep(2)\n",
    "        print(\" Rafiki >> Type bye or quit to end chat\")\n",
    "        time.sleep(3)\n",
    "        greeting  = np.random.choice([\"Hello I am Rafiki\",\"Hey Lets Talk\",\n",
    "                                     \"Niaje Rafiki,how are you\",\"Sasa,whats up??\"])\n",
    "        print(\"Rafiki >> \"+ greeting)\n",
    "    def user_input(self) :\n",
    "        text = input(\"Type something\")\n",
    "        if text.lower().strip() in ['bye','quit','exit'] :\n",
    "            self.end_chat = True\n",
    "            print(\"Rafiki>> Goodbye \")\n",
    "            time.sleep(2)\n",
    "            print(\" Rafiki >> Quitting Rafiki ,Asante for your patience\")\n",
    "        else :\n",
    "            # Here we tokenizer the text add the eos token and return the result as a pytorch tensor\n",
    "            self.new_user_input_ids = tokenizer.encode(text+tokenizer.eos_token,\\\n",
    "                                                       return_tensors ='pt')\n",
    "    def bot_responses(self) :\n",
    "        # check if this is a continuing chat or a new chat\n",
    "        if self.chat_history is not None :\n",
    "            # I will append/concatanate the chat_history id and the new_user_id as self.bot input ids\n",
    "            self.bot_input_ids = torch.cat([self.chat_history_ids,self.new_user_input_ids],dim=-1)\n",
    "        else :\n",
    "            self.bot_input_ids = self.new_user_input_ids\n",
    "            self.chat_history_ids = model.generate(self.bot_input_ids,max_length=1000,\\\n",
    "                                                  pad_token_id = tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(self.chat_history_ids[:,self.bot_input_ids.shape[-1]:][0],\\\n",
    "                                   skip_special_tokens = True)\n",
    "        # In the case Rafiki 'fails to answer'\n",
    "        if response == \"\" :\n",
    "            response = self.random_response()\n",
    "            print(\"Rafiki >>\"+ response)\n",
    "    def random_response(self) :\n",
    "        i = -1\n",
    "        response = tokenizer.decode(self.chat_history_ids[:,self_bot_input_ids[i]:][0],\\\n",
    "                                   skip_special_tokens=True )\n",
    "        while response ==\"\" :\n",
    "            response = tokenizer.decode(self.chat_history_ids[:,self_bot_input_ids[i]:][0],\\\n",
    "                                   skip_special_tokens=True )\n",
    "        if response.strip() == \"?\" :\n",
    "            reply = np.random.choice([\" I am not sure\",\"I do not know \",\"I have no idea\"])\n",
    "        else :\n",
    "            reply = np.random.choice([\"This is great I love our conversation\",\"Nice Talking to You\"])\n",
    "        return reply \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616e514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"global\" statement\n",
      "**********************\n",
      "\n",
      "   global_stmt ::= \"global\" identifier (\",\" identifier)*\n",
      "\n",
      "The \"global\" statement is a declaration which holds for the entire\n",
      "current code block.  It means that the listed identifiers are to be\n",
      "interpreted as globals.  It would be impossible to assign to a global\n",
      "variable without \"global\", although free variables may refer to\n",
      "globals without being declared global.\n",
      "\n",
      "Names listed in a \"global\" statement must not be used in the same code\n",
      "block textually preceding that \"global\" statement.\n",
      "\n",
      "Names listed in a \"global\" statement must not be defined as formal\n",
      "parameters or in a \"for\" loop control target, \"class\" definition,\n",
      "function definition, \"import\" statement, or variable annotation.\n",
      "\n",
      "**CPython implementation detail:** The current implementation does not\n",
      "enforce some of these restrictions, but programs should not abuse this\n",
      "freedom, as future implementations may enforce them or silently change\n",
      "the meaning of the program.\n",
      "\n",
      "**Programmerâ€™s note:** \"global\" is a directive to the parser.  It\n",
      "applies only to code parsed at the same time as the \"global\"\n",
      "statement. In particular, a \"global\" statement contained in a string\n",
      "or code object supplied to the built-in \"exec()\" function does not\n",
      "affect the code block *containing* the function call, and code\n",
      "contained in such a string is unaffected by \"global\" statements in the\n",
      "code containing the function call.  The same applies to the \"eval()\"\n",
      "and \"compile()\" functions.\n",
      "\n",
      "Related help topics: nonlocal, NAMESPACES\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f4999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

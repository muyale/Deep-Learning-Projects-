{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13274209",
   "metadata": {},
   "source": [
    "THE PURPOSE OF THIS PROJECT IS TO CREATE A MODEL USING LSTM THAT READS TEXT DATA AND CLASSIFIES IT AS EITHER HAM OR SPAM.\n",
    "DATA PREPROCESSING WILL BE DONE WITH THE HELP OF TORCH TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84577e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import torchtext\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Datasets\\\\smsspamcollection\\\\SMSSpamCollection\", sep='\\t',\n",
    "                       names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8a11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dd8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8998466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.histplot(data=df,x='length',color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9822a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d38616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field the Normal column and fieldlabel the label column\n",
    "import spacy\n",
    "spacy= spacy.load('en')\n",
    "TEXT = data.Field(tokenize=spacy,batch_first=True)\n",
    "LABEL = data.LabelField(dtpe=torch.float(),batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(\"type\",LABEL),(\"text\",TEXT)]\n",
    "training_data = data.TabularDataset(path=\"C:\\\\Datasets\\\\smsspamcollection\\\\SMSSpamCollection\",format='csv',fields=fields,skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64db7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data into training and validation set\n",
    "import random\n",
    "train_data,valid_data = random.split(split_ratio=0.75,random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961acc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary\n",
    "TEXT.build_vocab(train_data,min_freq=5)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# device agnostic code\n",
    "device ='cuda' if torch.cuda.is_available else 'cpu'\n",
    "batch_size = 64\n",
    "\n",
    "# using bucket iterator we will iterate through data to get batches of we need them\n",
    "train_iterator,valid_iterator = data.BucketIterator.split((train_data,valid_data),batch_size=batch_size,sort_key=\n",
    "                                                         lambda x:len(x.text),sort_within_batch=True,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78c9a7",
   "metadata": {},
   "source": [
    "# LSTM MODEL :: THE IDEA OF EMBEDDINGS+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b30fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self,vocabsize,emb_dim,hidden_dim,output_dim,\n",
    "                 num_layers,bidirectional,dropout):\n",
    "        super(TextClassifier,self)\n",
    "        self.embedding_dim = nn.Embedding(vocabsize,emb_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,num_layers=num_layers,\n",
    "                           bidirectional=bidirectional,dropout=dropout)\n",
    "        # an output layer which will be a linear layer\n",
    "        self.fc = nn.Linear(hidden_dim*2,output_dim)\n",
    "        # a sigmoid activation for our outputs \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,text,textlength):\n",
    "        embedded = self.embedding_dim(text)\n",
    "        # using padding sequence since LSTM rarely sees padding sequences\n",
    "        packed_embedding = nn.utils.rnn.packed_padded_sequence(embedded,\n",
    "                                                              textlength.cpu(),batch_first=True)\n",
    "        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)\n",
    "        # Concataneting the final output\n",
    "        hidden = torch.cat((hidden_state[:,:,2],hidden_state[:,:,-1]),dim=1)\n",
    "        dense_output = self.fc(hidden)\n",
    "        # applying our sigmoid function to the dense output to get a clear output\n",
    "        output = self.sigmoid(dense_output)\n",
    "        return output\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9de6ac",
   "metadata": {},
   "source": [
    "#HYPER PARAMETRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_VOCAB = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_HIDDEN_DIMS = 64\n",
    "NUM_OUTPUT_DIMS =1\n",
    "NUM_LAYERS =2\n",
    "BIDIRECTION = True\n",
    "DROPOUT =0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3fafa",
   "metadata": {},
   "source": [
    "# TRAINING AND TESTING OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(SIZE_OF_VOCAB,EMBEDDING_DIM,NUM_HIDDEN_DIMS,NUM_OUTPUT_DIMS,NUM_LAYERS,BIDIRECTION,DROPI)\n",
    "optimizer = torch.optim.Adam(model.parametres(),lr=0.001)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns the accuracy\n",
    "def binary_accuracy(preds,y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds ==y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "def train(model,iterator,optimizer,criterion):\n",
    "    epoch_acc = 0.0\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text,textlength = batch.text\n",
    "        predictions = model(text,textlength).squeeze()\n",
    "        loss = criterion(prediction,batch.type)\n",
    "        loss.backwards()\n",
    "        acc= binary_accuracy(predictions,batch.type)\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "        return epoch_loss/len(iterator),epoch_acc/len(iterator)\n",
    "def evaluate(model,iterator,optimizer,criterion):\n",
    "    epoch_loss=0.0\n",
    "    epoch_acc = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,textlenght = batch.text\n",
    "            predictions = model(text,textlenght).squeeze()\n",
    "            loss = criterion(predictions,batch.type)\n",
    "            acc = binary_accuracy(predictions,batch.type)\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc +=acc.item()\n",
    "            return epoch_loss/len(iterator),epoch_acc/len(iterator)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc = train(model,iterator,optimizer,criterion)\n",
    "    valid_loss,valid_acc = evaluate(model,iterator,optimizer,criterion)\n",
    "    print (f\"The train loss in {train_loss}|the train accuracy is {train_acc}%\")\n",
    "    print (f\"The validatation loss in {valid_loss}|the train accuracy is {valid_acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c2024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from nltk import word_tokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.rnn import pack_padded_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.preprocessing import.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae698956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparametres to be used later\n",
    "max_features= 120,000 # How many unique_words we are going to use\n",
    "embed_size = 300\n",
    "max_len = 750 # Length that each question is allowed to have\n",
    "batch_size =512\n",
    "n_epochs =5\n",
    "n_splits =5\n",
    "seed =10\n",
    "debug =0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff555ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/kuc-hackathon-winter-2018/drugsComTrain_raw.csv)\n",
    "test _df = pd.read_csv(\"../input/kuc-hackathon-winter-2018/drugsComTest_raw.csv\")\n",
    "data = pd.concat([train_df,test_df])[['review','condition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7564dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all the null values\n",
    "data = data[pd.notnull(data['review'])]\n",
    "# Creating a length variable\n",
    "data['lenght'] = data['review'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0050a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing our Target column\n",
    "# We are going to select all the conditions which have more than 3000 reviews\n",
    "count_df = data[['condition','review']].groupby('condition').aggregate({'review':'count'}).reset_index().sort_values('review',ascending=False)\n",
    "count_df.head()\n",
    "target_conditions = count_df[count_df['review']>3000['condition'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we already have selected the conditions with the 3000 and above ,we can then filter our dataframe\n",
    "\n",
    "def condition_parser(x):\n",
    "    # checks if a reviews meets the target condition criteria\n",
    "    if x in target_conditions:\n",
    "        return x\n",
    "    else :\n",
    "        return \"OTHER\"\n",
    "# Filtering our conditions column\n",
    "data['condition'] = data['condition'].apply(lambda x: condition_parser(x))\n",
    "data= data[data['condition']!='\"OTHER\"']\n",
    "# The values returned will have only the target conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7adc77c",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING\n",
    "In this section I will clean the data by removing numbers,patterns and contraditictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55018042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "def clean_text(x):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text.re.sub(pattern,\"\",x)\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d',x))):\n",
    "        x = re.sub('[0-9]{5,}',\"#####\",x)\n",
    "        x = re.sub('[0-9]{4}','####',x)\n",
    "        x = re.sub('[0-9]{3}','###',x)\n",
    "        x = re.sub('[0-9]{2}','##',x)\n",
    "        return x\n",
    "# Contradictions ,I will create a contradiction dictionary,then using regular expression extract the matches\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that preprocesses the text for  us\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = clean_text(text)\n",
    "    text = clean_numbers(text)\n",
    "    text = replace_contractions(text)\n",
    "    return text\n",
    "data['review'] =data['review'].apply(lambda x:preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our data into training and testing sets and then tokenizing them\n",
    "X= data['review']\n",
    "y = data['condition']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.25)\n",
    "# Tokenizing the X values\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts.to_sequence(X_train)\n",
    "X_test = tokenizer.texts.to_sequence(X_test)\n",
    "X_train = pad_sequences(X_train,maxlen=max_len)\n",
    "X_test = pad_sequences(X_test,maxlen = max_len)\n",
    "\n",
    "\n",
    "# Transforming the Y values \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train.values)\n",
    "y_test = le.fit_transform(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa50df",
   "metadata": {},
   "source": [
    "# LOADING GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe28197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    # I will load the glove pretrained model from Kaggle\n",
    "    EMBEDDING_FILE =   '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "    def get_coefs_(word,*arr):\n",
    "        return word,np.asarray(arr,dtype='float32')[:300]\n",
    "    embeddings_index =dict(get_coefs_(*o.split(\" \"))for o in open(EMBEDDING_FILE))\n",
    "    all_embs = np.stack(embeddins_index.values)\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size=all_embs.shape[1]\n",
    "    nb_words = min(max_features,len(word_index)+1)\n",
    "    embedding_matrix = np.random.normal(emb_mean,emb_std,(nb_words,embed_size))\n",
    "    for word,i in word_index.items():\n",
    "        if i >= max_features :continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "        else :\n",
    "            embedding_vector = embeddings_index.get(word.capitalize())\n",
    "            if embedding_vector is not None :\n",
    "                embedding_matrix (i) = embedding_vector\n",
    "    return embedding_matrix\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a87d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug :\n",
    "    embedding_matrix = np.random.randn(120000,300)\n",
    "    # This is to cover missing entries which are set using the above code\n",
    "else :\n",
    "    embedding_matrix = load_glove(tokenizer.word_index)\n",
    "#np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1d6a7",
   "metadata": {},
   "source": [
    "# BILSTM MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70692cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 64\n",
    "        drp =0.1\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding =nn.Embedding(max_features,embed_size)\n",
    "        self.embedding.weight =nn.Parameter(torch.tensor(embedding_matrix,dtype =torch.float32))\n",
    "        self.embedding.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_size,self.hidden_size,bidirection=True,batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*4,64)\n",
    "        self.relu = nn.Relu()\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.fc1 = nn.Linear(64,n_classes)\n",
    "    def forward(self,x):\n",
    "        h_embeddings = self.embeddings(x)\n",
    "        h_ltsm,_ = self.lstm(h_embeddings)\n",
    "        avg_pool = torch.mean(h_lstm,1)\n",
    "        max_pool,_ = torch.max(h_lstm,1)\n",
    "        concatenate = torch.cat((avg_pool,max_pool)1)\n",
    "        concatenate = self.relu(self.linear(concatenate))\n",
    "        concatenate = self.dropout(concatenate)\n",
    "        output = self.fc1(concatenate)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TRAINING AND EVALUATION\n",
    "model = BiLSTM()\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer= optim.Adam(filter(lambda p:p.requires_grad,model.parametres()),lr=0.001)\n",
    "# I WILL LOAD THE MODEL INTO CUDA ,Then create a dataset and dataloader with the use of torch.text\n",
    "model.cuda()\n",
    "X_train =torch.tensor(X_train,dtype=torch.long).cuda()\n",
    "X_test = torch.tensor(X_test,dtype=torch.long).cuda()\n",
    "X_cv = torch.tensor(X_test,dtype=torch.long).cuda()\n",
    "y_cv = torch.tensor(y_test,dtype=torch.long).cuda()\n",
    "\n",
    "# CREATING A DATASET AND DATALOADER\n",
    "from torch.utils.data import Tensor_Dataset,Tensor_Dataloader\n",
    "train = torch.utils.data.Tensor_Dataset(X_train,y_train)\n",
    "valid = torch.utils.data.Tensor_Dataset(X_test,y_test)\n",
    "# creating a dataloader\n",
    "train_loader = torch.utils.Dataloader(train,batch_size=batch_size,shuffle=True)\n",
    "valid_loader = torch.utils.Dataloader(valid,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa169016",
   "metadata": {},
   "source": [
    "# Training and Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    avg_loss = 0.0\n",
    "    for i,(x.batch,y.batch) in enumerate(train_loader):\n",
    "        y_pred = model(x.batch)\n",
    "        loss =loss_fn(y_pred,y.batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss+=loss.item()/len(train_loader)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for i ,(x.batch,y.batch) in enumerate(valid_loader):\n",
    "                avg_val_loss = 0\n",
    "                y_pred = model(x.batch)\n",
    "                valid_loss = loss_fn(y_pred,y.batch)\n",
    "                train_loss.append(loss)\n",
    "                valid_loss.append(valid_loss)\n",
    "                print(f\"Epoch{epoch}|train_loss:{train_loss}|valid_loss:{valid_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db118dc",
   "metadata": {},
   "source": [
    "# PREDICTIVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998572d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(text):\n",
    "    text = preprocess_text(text)\n",
    "    text = tokenizer.texts_to_sequence(text)\n",
    "    text = pad_sequences(text,maxlen=max_len)\n",
    "    text = torch.tensor(text,dtype=torch.long).cuda()\n",
    "    pred = model(text).detach()\n",
    "    pred = F.softmax(pred).cpu().numpy()\n",
    "    pred = pred.argmax(axis=1)\n",
    "    pred =le.classes_[pred]\n",
    "    return pred[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

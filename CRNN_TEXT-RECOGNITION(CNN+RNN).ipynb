{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import multiprocessing as mp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0507c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60954aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/kaggle/input/captcha-version-2-images/samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fns = os.listdir(data_path)\n",
    "# remove samples\n",
    "image_fns.remove('samples')\n",
    "image_fns_train,image_fns_test = train_test_split(image_fn,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1634d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Maps\n",
    "image_ns = [image_fn.split('.')[0] for image_fn in image_fns]\n",
    "image_ns =''.join(image_ns)\n",
    "letters = sorted(list(set(list(image_ns))))\n",
    "vocabulary = ['-']+ letters\n",
    "idx2char = {k:v for k,v in enumerate(vocabulary,state=0)}\n",
    "char2idx = {v:k for k,v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805e4c4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4225546505.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\EDGAR MUYALE DAVIES\\AppData\\Local\\Temp\\ipykernel_1704\\4225546505.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    class CaptchaDataset(Dataset):\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# DataLoader \n",
    "batch_size=16\n",
    "\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self,data_dir,image_fns):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_fns = image_fns\n",
    "    def __len__(self):\n",
    "        return len(self.image_fns)\n",
    "    def __getitem(self,index):\n",
    "        image_fn = self.image_fn[index]\n",
    "        image_fp = ospath.join(self,data_dir,image_fn)\n",
    "        image = Image.open(image_fp).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        text = image_fn.split('.')[0]\n",
    "        return image,text\n",
    "    def transform(self,image):\n",
    "        transform_ops = transforms.compose([transforms.ToTensor(),transforms.Nomalize(\n",
    "        mean=(0.485,0.456,0.406),std=(0.229,0.224,0.225))])\n",
    "        return transform_ops(image)\n",
    "    \n",
    "    # Train and Test datasets\n",
    "    trainset = CaptchaDataset(data_dir,image_fns_train)\n",
    "    testset = CaptchaDataset(data_dir,image_fns_test)\n",
    "     # Loading the datasets\n",
    "    train_loader = Dataloader(trainset,batch_size=batch_size,shuffle=True)\n",
    "    test_loader = Dataloader(testset,batch_size=batch_size,shuffle=False)\n",
    "    # Setting an iteration \n",
    "    train_iterator,valid_iterator = iter(train_loader.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc0055",
   "metadata": {},
   "source": [
    "# CRNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dddf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some of the parameters of the model\n",
    "num_chars = len(char2idx)\n",
    "rnn_hidden_size = 256\n",
    "resnet = resnet18(pretrained=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6871cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self,num_chars,rnn_hidden_size=256,dropout=0.2):\n",
    "        super(CRNN).__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.dropout =dropout\n",
    "        resnet_modules = list(resnet.children()[:-3])\n",
    "        self.cnn1 = nn.Sequential(*resnet_modules)\n",
    "        self.cnn2 = nn.Sequential(nn.Conv2d(256,256,kernel_size=(3,6),stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(256),nn.Relu(inplace=True))\n",
    "        self.linear1 = nn.Linear(rnn_hidden_size,rnn_hidden_size)\n",
    "        self.rnn1 = nn.GRU(rnn_hidden_size,rnn_hidden_size,bidirection=True,dropout=dropout,batch_first=True)\n",
    "        self.rnn2 = nn.GRU(rnn_hidden_size,rnn_hidden_size,bidirection=True,dropout=dropout,batch_first=0)\n",
    "        self.linear2 = nn.Linear(rnn_hidden_size,num_chars)\n",
    "    def forward(self,batch):\n",
    "        batch = self.cnn1(batch)\n",
    "        batch = self.cnn2(batch)\n",
    "        batch =batch.permute(0,3,1,2)\n",
    "        batch_size1 = batch.size(0)\n",
    "        batch_size2 = batch.size(1)\n",
    "        batch = batch.view(batch_size1,batch_size2)\n",
    "        batch = self.linear1(batch)\n",
    "        batch,hidden = self.rnn1(batch)\n",
    "        feature_size =batch.size(2)\n",
    "        batch = batch[:,:,:feature_size//2]+batch[:,:,feature_size//2]\n",
    "        batch,hidden = self.rnn2(batch)\n",
    "        batch = self.linear2(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95f59b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that iniatializes the weights for our model\n",
    "def weight_initializer(m):\n",
    "    class_name = __class__.__name__\n",
    "    if type(m) in [nn.Linear,nn.Conv2d,nn.Conv1d]:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "        elif class_name.find('BatchNorm')!=-1:\n",
    "            m.weight_data.normal_(1.0,0.02)\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =CRNN(num_chars,rnn_hidden_size)\n",
    "text_batch_logits = model(image_batch).to(device)\n",
    "criterion = nn.CTCLoss(blank=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "970c3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that encodes text\n",
    "def encodes_text(text_batch):\n",
    "    text_batch_target_lens = [len(text)for text in text_batch]\n",
    "    text_batch_target_lens = torch.IntTensor(text_batch_target_lens)\n",
    "    text_batch_concat = ''.join(text_batch)\n",
    "    text_batch_targets = [char2idx[c] for c in text_batch_concat]\n",
    "    text_batch_targets = torch.IntTensor(text_batch_targets)\n",
    "    return text_batch_targets,text_batch_target_lens\n",
    "# create a function that computes our loss\n",
    "def compute_loss(text_batch,text_batch_logits):\n",
    "    text_batch_logPs = F.log_softmax(text_batch_logits,2)\n",
    "    text_batch_logPs_lens = torch.full(size=(text_batch_logPs.size(1)),\n",
    "                                      fill_value = text_batch_logPs.size(0),\n",
    "                                      dtype= torch.Int32).to(device)\n",
    "    text_batch_targets,text_batch_target_lens = encodes_text(text_batch)\n",
    "    loss = Criterion(text_batch_logPs,text_batch_targets,text_batch_logPs_lens,text_batch_target_lens)\n",
    "    return loss\n",
    "compute_loss(text_batch,text_batch_logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c907873",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 1e-3\n",
    "weigth_decay = 1e-3\n",
    "clip_norm =5\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "lr_scheduler =optim.lr_scheduler.ReduceLROnPlateau(optimizer,verbose=True,patience=5)\n",
    "model\n",
    "model.apply(weight_initializer)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d48ed",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = []\n",
    "iterator_loss = []\n",
    "for epoch in tqdm(range(1,epochs+1)):\n",
    "    epoch_loss_list = []\n",
    "    num_updates_epochs = []\n",
    "    for image_batch,text_batch in tqdm(train_loader,leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        text_batch_logits = model(image_batch)\n",
    "        loss = compute_loss(text_batch,text_batch_logits)\n",
    "        iterator_loss+=loss.item()\n",
    "        if np.isnan(iterator_loss) :\n",
    "            continue\n",
    "        num_updates_epochs +=1\n",
    "        iterator_loss.append(iterator_loss)\n",
    "        loss.backward()\n",
    "        nn.util.clip_grad_norm(model.parameters(),clip_norm)\n",
    "        optimizer.step()\n",
    "        epoch_loss =np.mean(epoch_loss_list)\n",
    "        num_updates_epochs.append(num_updates_epochs)\n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc569447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that makes predictions given text data \n",
    "def make_predictions(text_batch_logits):\n",
    "    text_batch_tokens =F.Softmax(text_batch_logits,2).argmax(2)\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "        \n",
    "    return text_batch_tokens_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2c7c0",
   "metadata": {},
   "source": [
    "Credits to\n",
    ": : https://github.com/GokulKarthik/deep-learning-projects-pytorch\n",
    "            ::  https://github.com/carnotaur/crnn-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c326b0",
   "metadata": {},
   "source": [
    "# THE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a0508a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
